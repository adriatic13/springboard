{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Adrian Marinovich\n",
    "# Springboard - Data Science Career Track \n",
    "# Capstone Project #2\n",
    "# Feature extraction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from collections import deque\n",
    "import csv\n",
    "import cv2\n",
    "import glob\n",
    "import numpy as np\n",
    "import operator\n",
    "import os\n",
    "import os.path\n",
    "import pickle\n",
    "import random\n",
    "import shutil\n",
    "import sys\n",
    "import threading\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "from scipy.stats import reciprocal, uniform\n",
    "from scipy.misc import imsave\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "# for reproducibility:\n",
    "np.random.seed(41)\n",
    "tf.reset_default_graph()\n",
    "tf.set_random_seed(51)\n",
    "random.seed(61)\n",
    "os.environ['PYTHONHASHSEED'] = '0'\n",
    "    \n",
    "import keras\n",
    "from keras.applications.inception_v3 import InceptionV3, preprocess_input\n",
    "from keras.applications import VGG16\n",
    "from keras import backend as K\n",
    "from keras.callbacks import TensorBoard, ModelCheckpoint, EarlyStopping, CSVLogger\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import Input\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import ZeroPadding3D\n",
    "from keras.layers.convolutional import (Conv2D, MaxPooling3D, Conv3D,\n",
    "    MaxPooling2D)\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.layers.recurrent import GRU\n",
    "from keras.layers.wrappers import TimeDistributed\n",
    "from keras.models import Model, load_model\n",
    "from keras.models import model_from_yaml\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import SGD, Adam, RMSprop\n",
    "from keras.preprocessing import image\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.preprocessing.image import img_to_array, load_img\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "from sklearn.svm import LinearSVC \n",
    "from sklearn.svm import SVC \n",
    "\n",
    "from sklearn.datasets import fetch_mldata\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.multiclass import OneVsOneClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "# setup plots\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['axes.labelsize'] = 14\n",
    "plt.rcParams['xtick.labelsize'] = 12\n",
    "plt.rcParams['ytick.labelsize'] = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapted from:\n",
    "#   https://github.com/harvitronix/five-video-classification-methods\n",
    "#   https://github.com/wushidonguc/two-stream-action-recognition-keras\n",
    "#   https://github.com/fchollet/deep-learning-with-python-notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (adapted from:\n",
    "#   https://github.com/harvitronix/five-video-classification-methods/blob/master/processor.py )\n",
    "\n",
    "# Process image and return array\n",
    "\n",
    "def process_image(image, target_shape):\n",
    "    # Load the image.\n",
    "    h, w, _ = target_shape\n",
    "    image = load_img(image, target_size=(h, w))\n",
    "\n",
    "    # Turn it into numpy, normalize and return.\n",
    "    img_arr = img_to_array(image)\n",
    "    x = (img_arr / 255.).astype(np.float32)\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (adapted from:\n",
    "#    https://github.com/harvitronix/five-video-classification-methods/blob/master/data.py\n",
    "#    https://github.com/wushidonguc/two-stream-action-recognition-keras/blob/master/spatial_train_data.py )\n",
    "\n",
    "class threadsafe_iterator:\n",
    "    def __init__(self, iterator):\n",
    "        self.iterator = iterator\n",
    "        self.lock = threading.Lock()\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        with self.lock:\n",
    "            return next(self.iterator)\n",
    "\n",
    "def threadsafe_generator(func):\n",
    "    # Decorator\n",
    "    def gen(*a, **kw):\n",
    "        return threadsafe_iterator(func(*a, **kw))\n",
    "    return gen\n",
    "\n",
    "class DataSet():\n",
    "\n",
    "    os.chdir('/home/adrian01/ucf101')\n",
    "    \n",
    "    def __init__(self, seq_length=0, class_limit=None, image_shape=(299, 299, 3)):\n",
    "        # Constructor\n",
    "        #   seq_length = (int) number of frames for fixed sequence length, or:\n",
    "        #     0 ~> variable sequence length\n",
    "        #   class_limit = (int) number of classes to limit the data to, or:\n",
    "        #     None ~> no limit.\n",
    "\n",
    "        # Set directory name where to put features once extracted\n",
    "        #   Currently these are in place:\n",
    "        #     features01 ~> features for 50-frame fixed sequences\n",
    "        #     features02 ~> features for 80-frame fixed sequences\n",
    "        #     features03 ~> features for 100-frame fixed sequences\n",
    "        #     features04 ~> features for variable length sequences\n",
    "        self.sequence_path = os.path.join('/home/adrian01/ucf101', 'features04')\n",
    "        \n",
    "        self.seq_length = seq_length\n",
    "        self.class_limit = class_limit\n",
    "        self.min_frames = 2  # min number of frames a video can have for us to use it\n",
    "        self.max_frames = 2000  # max number of frames a video can have for us to use it\n",
    "\n",
    "        # Get data\n",
    "        self.data = self.get_data()\n",
    "\n",
    "        # Get classes\n",
    "        self.classes = self.get_classes()\n",
    "\n",
    "        # Limit within min and max number of frames\n",
    "        self.data = self.clean_data()\n",
    "        self.image_shape = image_shape\n",
    "\n",
    "    @staticmethod\n",
    "    def get_data():\n",
    "        # Load list of video metadata as a list \n",
    "        with open(os.path.join('/home/adrian01/data_file.csv'), 'r') as fin:\n",
    "            reader = csv.reader(fin)\n",
    "            data = list(reader)\n",
    "\n",
    "        return data\n",
    "\n",
    "    def clean_data(self):\n",
    "        # Limit within min and max number of frames\n",
    "        #  and to specified classes\n",
    "        data_clean = []\n",
    "        for item in self.data:\n",
    "            if int(item[3]) >= self.min_frames and int(item[3]) <= self.max_frames \\\n",
    "                    and item[1] in self.classes:\n",
    "                data_clean.append(item)\n",
    "\n",
    "        return data_clean\n",
    "\n",
    "    def get_classes(self):\n",
    "        # Extract classes, and with specified limit\n",
    "        classes = []\n",
    "        for item in self.data:\n",
    "            if item[1] not in classes:\n",
    "                classes.append(item[1])\n",
    "\n",
    "        # Sort\n",
    "        classes = sorted(classes)\n",
    "\n",
    "        # Return\n",
    "        if self.class_limit is not None:\n",
    "            return classes[:self.class_limit]\n",
    "        else:\n",
    "            return classes\n",
    "\n",
    "    def get_class_one_hot(self, class_str):\n",
    "        # Given class as string, return its number in classes list\n",
    "        #  This lets us encode and one-hot it for training.\n",
    "        \n",
    "        # Encode it first\n",
    "        label_encoded = self.classes.index(class_str)\n",
    "\n",
    "        # Then one-hot it\n",
    "        label_hot = to_categorical(label_encoded, len(self.classes))\n",
    "\n",
    "        assert len(label_hot) == len(self.classes)\n",
    "\n",
    "        return label_hot\n",
    "\n",
    "    def split_train_test(self):\n",
    "        # Split the data into train and test groups\n",
    "        \n",
    "        train = []\n",
    "        test = []\n",
    "        for item in self.data:\n",
    "            if item[0] == 'train':\n",
    "                train.append(item)\n",
    "            else:\n",
    "                test.append(item)\n",
    "        return train, test\n",
    "\n",
    "    def get_all_sequences_in_memory(self, train_test, data_type):\n",
    "        # This is like our generator, but attempts to load everything \n",
    "        #   into memory, to train faster\n",
    "\n",
    "        # Get the dataset\n",
    "        train, test = self.split_train_test()\n",
    "        data = train if train_test == 'train' else test\n",
    "\n",
    "        print(\"Loading %d samples into memory for %sing.\" % (len(data), train_test))\n",
    "\n",
    "        X, y = [], []\n",
    "        for row in data:\n",
    "\n",
    "            if data_type == 'images':\n",
    "                frames = self.get_frames_for_sample(row)\n",
    "                frames = self.rescale_list(frames, self.seq_length)\n",
    "\n",
    "                # Build the image sequence\n",
    "                sequence = self.build_image_sequence(frames)\n",
    "\n",
    "            else:\n",
    "                sequence = self.get_extracted_sequence(data_type, row)\n",
    "\n",
    "                if sequence is None:\n",
    "                    print(\"Can't find sequence. Did you generate them?\")\n",
    "                    raise\n",
    "\n",
    "            X.append(sequence)\n",
    "            y.append(self.get_class_one_hot(row[1]))\n",
    "\n",
    "        return np.array(X), np.array(y)\n",
    "\n",
    "    @threadsafe_generator\n",
    "    def frame_generator(self, batch_size, train_test, data_type):\n",
    "        # Makes a generator to train on,\n",
    "        #  returning either data_type: \n",
    "        #    'features', 'images'\n",
    "\n",
    "        # Get dataset for the generator\n",
    "        train, test = self.split_train_test()\n",
    "        data = train if train_test == 'train' else test\n",
    "\n",
    "        print(\"Creating %s generator with %d samples.\" % (train_test, len(data)))\n",
    "\n",
    "        while 1:\n",
    "            X, y = [], []\n",
    "\n",
    "            # Generate batch_size samples\n",
    "            for _ in range(batch_size):\n",
    "                # Reset to be safe\n",
    "                sequence = None\n",
    "\n",
    "                # Get a random sample\n",
    "                sample = random.choice(data)\n",
    "\n",
    "                # Check to see if we've already saved this sequence\n",
    "                if data_type is \"images\":\n",
    "                    # Get and resample frames\n",
    "                    frames = self.get_frames_for_sample(sample)\n",
    "                    frames = self.rescale_list(frames, self.seq_length)\n",
    "\n",
    "                    # Build the image sequence\n",
    "                    sequence = self.build_image_sequence(frames)\n",
    "                else:\n",
    "                    # Get the sequence from disk\n",
    "                    sequence = self.get_extracted_sequence(data_type, sample)\n",
    "\n",
    "                    if sequence is None:\n",
    "                        raise ValueError(\"Can't find sequence. Did you generate them?\")\n",
    "\n",
    "                X.append(sequence)\n",
    "                y.append(self.get_class_one_hot(sample[1]))\n",
    "\n",
    "            yield np.array(X), np.array(y)\n",
    "\n",
    "    def build_image_sequence(self, frames):\n",
    "        # Given a set of frames (filenames), build sequence\n",
    "        return [process_image(x, self.image_shape) for x in frames]\n",
    "\n",
    "    def get_extracted_sequence(self, data_type, sample):\n",
    "        # Get saved extracted features\n",
    "        filename = sample[2]\n",
    "        path = os.path.join(self.sequence_path, filename + '-' + str(self.seq_length) + \\\n",
    "            '-' + data_type + '.npy')\n",
    "        if os.path.isfile(path):\n",
    "            return np.load(path)\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    def get_frames_by_filename(self, filename, data_type):\n",
    "        # Given a filename for one of our samples, return the data\n",
    "        \n",
    "        # First, find the sample row.\n",
    "        sample = None\n",
    "        for row in self.data:\n",
    "            if row[2] == filename:\n",
    "                sample = row\n",
    "                break\n",
    "        if sample is None:\n",
    "            raise ValueError(\"Couldn't find sample: %s\" % filename)\n",
    "\n",
    "        if data_type == \"images\":\n",
    "            # Get and resample frames.\n",
    "            frames = self.get_frames_for_sample(sample)\n",
    "            frames = self.rescale_list(frames, self.seq_length)\n",
    "            # Build the image sequence\n",
    "            sequence = self.build_image_sequence(frames)\n",
    "        else:\n",
    "            # Get the sequence from disk.\n",
    "            sequence = self.get_extracted_sequence(data_type, sample)\n",
    "\n",
    "            if sequence is None:\n",
    "                raise ValueError(\"Can't find sequence. Did you generate them?\")\n",
    "\n",
    "        return sequence\n",
    "\n",
    "    @staticmethod\n",
    "    def get_frames_for_sample(sample):\n",
    "        # Given a video row from video metadata file, get corresponding frame filenames\n",
    "        path = os.path.join('/home/adrian01/ucf101', sample[0], sample[1])\n",
    "        filename = sample[2]\n",
    "        images = sorted(glob.glob(os.path.join(path, filename + '*jpg')))\n",
    "        return images\n",
    "\n",
    "    @staticmethod\n",
    "    def get_filename_from_image(filename):\n",
    "        parts = filename.split(os.path.sep)\n",
    "        return parts[-1].replace('.jpg', '')\n",
    "\n",
    "    @staticmethod\n",
    "    def rescale_list(input_list, size):\n",
    "        # Return list of frames at specified fixed sequence length, or:\n",
    "        #   Note that size==0 will cause output of entire image sequence\n",
    "        \n",
    "        if size == 0:\n",
    "            outpt = [input_list[i] for i in range(0, len(input_list))]\n",
    "            return outpt\n",
    "        \n",
    "        elif len(input_list) > size:\n",
    "            skip = len(input_list) // size\n",
    "            outpt = [input_list[i] for i in range(0, len(input_list), skip)]\n",
    "            return outpt[:size]\n",
    "\n",
    "        elif len(input_list) == size:\n",
    "            outpt = [input_list[i] for i in range(0, len(input_list))]\n",
    "            return outpt[:size]\n",
    "\n",
    "        elif len(input_list) < size and len(input_list) > 2:\n",
    "            add = 1 - (len(input_list) / size)\n",
    "            a_c = 0\n",
    "            x_c = 0\n",
    "            outpt = []\n",
    "            list_end = len(input_list) - 1\n",
    "            for x in range(size):\n",
    "                a_c += add\n",
    "                x_c = np.ceil(x - a_c)\n",
    "                x_c = x_c.astype(np.int16)\n",
    "                if x_c <= list_end:\n",
    "                    outpt.append(input_list[x_c])\n",
    "                elif x_c > list_end:\n",
    "                    outpt.append(input_list[list_end])\n",
    "            return outpt[:size]\n",
    "\n",
    "        else:\n",
    "            outpt = []\n",
    "            for x in range(size):\n",
    "                outpt.append(input_list[0])\n",
    "            return outpt[:size]\n",
    "    \n",
    "    def print_class_from_prediction(self, predictions, nb_to_return=5):\n",
    "        # Given a prediction, print the top classes\n",
    "\n",
    "        # Get the prediction for each label\n",
    "        label_predictions = {}\n",
    "        for i, label in enumerate(self.classes):\n",
    "            label_predictions[label] = predictions[i]\n",
    "\n",
    "        # Sort\n",
    "        sorted_lps = sorted(\n",
    "            label_predictions.items(),\n",
    "            key=operator.itemgetter(1),\n",
    "            reverse=True\n",
    "        )\n",
    "\n",
    "        # And return the top N\n",
    "        for i, class_prediction in enumerate(sorted_lps):\n",
    "            if i > nb_to_return - 1 or class_prediction[1] == 0.0:\n",
    "                break\n",
    "            print(\"%s: %.2f\" % (class_prediction[0], class_prediction[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (adapted from:\n",
    "#    https://github.com/harvitronix/five-video-classification-methods/blob/master/extractor.py )\n",
    "\n",
    "class Extractor():\n",
    "    \n",
    "    os.chdir('/home/adrian01/ucf101')\n",
    "    \n",
    "    def __init__(self, weights=None):\n",
    "        # Either load pretrained from imagenet, or \n",
    "        #   load saved weights from our own training\n",
    "\n",
    "        self.weights = weights  # so we can check elsewhere which model\n",
    "\n",
    "        if weights is None:\n",
    "            # Get model with pretrained weights\n",
    "            base_model = InceptionV3(\n",
    "                weights='imagenet',\n",
    "                include_top=True\n",
    "            )\n",
    "            \n",
    "            # Extract features at the final pool layer.\n",
    "            self.model = Model(\n",
    "                inputs=base_model.input,\n",
    "                outputs=base_model.get_layer('avg_pool').output\n",
    "            )            \n",
    "            \n",
    "        else:\n",
    "            # Load the model first.\n",
    "            self.model = load_model(weights)\n",
    "\n",
    "            # Then remove the top so we get features, not predictions\n",
    "            # From: https://github.com/fchollet/keras/issues/2371\n",
    "            self.model.layers.pop()\n",
    "            self.model.layers.pop()  # two pops to get to pool layer\n",
    "            self.model.outputs = [self.model.layers[-1].output]\n",
    "            self.model.output_layers = [self.model.layers[-1]]\n",
    "            self.model.layers[-1].outbound_nodes = []\n",
    "\n",
    "    def extract(self, image_path):\n",
    "        img = load_img(image_path, target_size=(299, 299))\n",
    "        x = img_to_array(img)\n",
    "        x = np.expand_dims(x, axis=0)\n",
    "        x = preprocess_input(x)\n",
    "\n",
    "        # Get the prediction\n",
    "        features = self.model.predict(x)\n",
    "\n",
    "        if self.weights is None:\n",
    "            # For imagenet/default network:\n",
    "            features = features[0]\n",
    "        else:\n",
    "            # For loaded network:\n",
    "            features = features[0]\n",
    "\n",
    "        return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13320/13320 [17:15:33<00:00,  4.55s/it]   \n"
     ]
    }
   ],
   "source": [
    "# DONE - DO NOT RUN\n",
    "# COMPLETED WITH 13320 ITERATIONS - 50-frame length in /features01\n",
    "#                                   80-frame length in /features02\n",
    "#                                   100-frame length in /features03\n",
    "#                                   variable-length features in /features04\n",
    "#\n",
    "# (adapted from:\n",
    "#    https://github.com/harvitronix/five-video-classification-methods/blob/master/extract_features.py )\n",
    "\n",
    "# Generates extracted features for each video, bundled into array \n",
    "#   with (sequence length, 2048) dimension\n",
    "\n",
    "\n",
    "# Class_limit is an integer that denotes the first N classes you want to extract features from\n",
    "\n",
    "# Set defaults.\n",
    "\n",
    "os.chdir('/home/adrian01/ucf101')\n",
    "\n",
    "seq_length = 0\n",
    "class_limit = None  # Number of classes to extract. Can be 1-101 or None for all.\n",
    "\n",
    "# Get the dataset.\n",
    "data = DataSet(seq_length=seq_length, class_limit=class_limit)\n",
    "\n",
    "# get the model.\n",
    "model = Extractor()\n",
    "\n",
    "# Loop through data.\n",
    "pbar = tqdm(total=len(data.data))\n",
    "for video in data.data:\n",
    "\n",
    "    # Get the path to the sequence for this video.\n",
    "    path = os.path.join('/home/adrian01/ucf101', 'features04', video[2] + '-' + str(video[3]) + \\\n",
    "        '-features')  # numpy will auto-append .npy\n",
    "\n",
    "    # Check if we already have it.\n",
    "    if os.path.isfile(path + '.npy'):\n",
    "        pbar.update(1)\n",
    "        continue\n",
    "\n",
    "    # Get the frames for this video.\n",
    "    frames = data.get_frames_for_sample(video)\n",
    "\n",
    "    # Now downsample to just the ones we need.\n",
    "    frames = data.rescale_list(frames, seq_length)\n",
    "\n",
    "    # Now loop through and extract features to build the sequence.\n",
    "    sequence = []\n",
    "    for image in frames:\n",
    "        features = model.extract(image)\n",
    "        sequence.append(features)\n",
    "\n",
    "    # Save the sequence.\n",
    "    np.save(path, sequence)\n",
    "\n",
    "    pbar.update(1)\n",
    "\n",
    "pbar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(160, 2048)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chek = np.load('/home/adrian01/ucf101/features04/v_Swing_g08_c01-0-features.npy')\n",
    "chek.shape\n",
    "\n",
    "#should be (160, 2048)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN WHEN DONE:\n",
    "#K.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
